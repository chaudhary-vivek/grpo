{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e600e1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import reasoning_gym\n",
    "import torch\n",
    "from rich import print\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7cd90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'HuggingfaceTB/SmolLM-135M-Instruct'\n",
    "\n",
    "llm = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e41a445",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_response(messages):\n",
    "    chat_template = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    inputs = tokenizer(chat_template, return_tensors='pt')\n",
    "    print(f\"Input ids shape: {inputs['input_ids'].shape}, first 10 tokens: {inputs['input_ids'][0, :10]}\")\n",
    "    outputs = llm.generate(**inputs, max_new_tokens=100)\n",
    "    input_length = inputs['input_ids'].shape[1]\n",
    "    newly_generated_tokens = outputs[:, input_length:]\n",
    "\n",
    "    decoded = tokenizer.batch_decode(newly_generated_tokens)[0]\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565ac3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_name = 'propositional_logic'\n",
    "dataset = reasoning_gym.create_dataset(environment_name, seed=SEED, size =5)\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "generate an answer after thinking.\n",
    "Use <think> your reasons here </think> <answer> Answer here </answer>\n",
    "You must answer within the <answer>...</answer> !\n",
    "\"\"\"\n",
    "\n",
    "def extract_answer(response):\n",
    "    # Simple regex search to extract answers\n",
    "    answer = re.search(r'answer>(.*?)</answer>', response, re.DOTALL)\n",
    "    if answer is not None:\n",
    "        return answer.group(1)\n",
    "    return answer\n",
    "\n",
    "\n",
    "for example in dataset:\n",
    "    question = example['question']\n",
    "    answer = example['answer']\n",
    "\n",
    "    print(f\"[bold white]System : {system_prompt}[/bold white]\")\n",
    "    print(f\"[bold blue]Question : [/bold blue]\\n\" + question)\n",
    "    if answer is not None:\n",
    "        print(f\"[bold green]Answer : [/bold green]\\n\" + answer)\n",
    "\n",
    "    llm_response = input(\"Let's say the LLM response is\")\n",
    "    # llm_response = 'lol'\n",
    "\n",
    "    answer = extract_answer(llm_response)\n",
    "    score_func = reasoning_gym.get_score_answer_fn(example[\"metadata\"]['source_dataset'])\n",
    "\n",
    "    print(f\"Extracted answer: \", answer)\n",
    "    reward = score_func(answer, example)\n",
    "\n",
    "    if reward > 0:\n",
    "        print(f\"[bold yellow]Reward : [/bold yellow] {reward}\")\n",
    "    else:\n",
    "        print(f\"[bold red]Incorrect\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061dcca1",
   "metadata": {},
   "source": [
    "### There are two phases\n",
    "* Data collection phase\n",
    "  * Environment -> LLM -> Buffer\n",
    "* Training Phase\n",
    "  * Experience buffer <-> Policy optimization <-> LLM\n",
    "\n",
    "### Four things go into the experience buffer\n",
    "* Full sequence : prompt and answer\n",
    "* Binary reponse mask\n",
    "* Group relative advantage of the response\n",
    "* Log probability of generated tokens themselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb582e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d6623bfd5df4973a20da415979c3730",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/723 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vivekchaudhary/anaconda3/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/vivekchaudhary/anaconda3/lib/python3.11/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <C1CC76AA-CD55-3E10-9064-29676E3E2535> /Users/vivekchaudhary/anaconda3/lib/python3.11/site-packages/torchvision/image.so\n",
      "  Expected in:     <B6BD92AE-4D03-3F92-9E03-2E2594A12866> /Users/vivekchaudhary/anaconda3/lib/python3.11/site-packages/torch/lib/libtorch_cpu.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5efbec04f484f0b9db791eaccb2c7d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/269M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac39a8c4fc8c4d8285e704f6ec10d2bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/156 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
