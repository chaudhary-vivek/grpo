{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from accelerate import Accelerator\n",
    "from utils import *\n",
    "import grpo_utils\n",
    "\n",
    "model_name = \"HuggingFaceTB/SmolLM-135M-Instruct\"\n",
    "batch_size = 2\n",
    "n_rollouts = 3\n",
    "buffer_size = 6\n",
    "max_new_tokens = 100\n",
    "\n",
    "\n",
    "# load essentials\n",
    "llm = load_model(model_name) # For full finetuning\n",
    "# llm = load_peft_model(model_name) # For only lora weights training\n",
    "tokenizer = load_tokenizer(model_name)\n",
    "dataloader = get_dataloader(\"syllogism\", tokenizer)\n",
    "optimizer = torch.optim.Adam(llm.parameters(), lr=1e-5)\n",
    "\n",
    "\n",
    "# Initialize accelerator\n",
    "accelerator = Accelerator()\n",
    "llm, tokenizer, dataloader, optimizer = accelerator.prepare(\n",
    "    llm, tokenizer, dataloader, optimizer\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dataloader))\n",
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch['validator']\n",
    "# batch['inputs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch[\"inputs\"][\"attention_mask\"]\n",
    "batch[\"inputs\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(batch[\"inputs\"][\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tokenizer.batch_decode(batch[\"inputs\"][\"input_ids\"])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch[\"validator\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dataloader))\n",
    "input_ids = batch[\"inputs\"][\"input_ids\"]\n",
    "attention_mask = batch[\"inputs\"][\"attention_mask\"]\n",
    "validator = batch[\"validator\"]\n",
    "input_size = input_ids.shape[1]\n",
    "\n",
    "with torch.no_grad():\n",
    "    full_responses = llm.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=max_new_tokens, # 100\n",
    "        do_sample=True,\n",
    "        top_p=0.95,\n",
    "        num_return_sequences=n_rollouts,\n",
    "        temperature=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    \n",
    "    assistant_responses = full_responses[:, input_size:]\n",
    "\n",
    "    # Calculate the logits for each selected tokens\n",
    "    log_probs = grpo_utils.calculate_logits(llm, full_responses, attention_mask)\n",
    "\n",
    "    # Convert tokens to string\n",
    "    decoded_responses = tokenizer.batch_decode(\n",
    "        assistant_responses, skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    # model_responses = [batch_size*n_rollouts, max_new_tokens]\n",
    "    rewards = grpo_utils.calculate_rewards(\n",
    "        decoded_responses, np.repeat(validator, n_rollouts)\n",
    "    )\n",
    "\n",
    "    # advantages = [batch_size, n_rollouts]\n",
    "    rewards = np.reshape(rewards, [batch_size, n_rollouts])\n",
    "    advantages = (rewards - np.mean(rewards, axis=1, keepdims=True)) / (\n",
    "        np.std(rewards, axis=1, keepdims=True) + 1e-8\n",
    "    )\n",
    "\n",
    "    advantages = advantages.reshape(-1, 1)\n",
    "    advantages = torch.tensor(advantages, dtype=torch.float32).to(llm.device)\n",
    "\n",
    "    padded_tokens = (full_responses != tokenizer.eos_token_id).int()\n",
    "    response_start_idx = padded_tokens.argmax(axis=-1)\n",
    "    response_end_idx = padded_tokens.shape[1] - torch.flip(\n",
    "        padded_tokens, dims=[1]\n",
    "    ).argmax(dim=1)\n",
    "\n",
    "    response_mask = torch.zeros_like(padded_tokens)\n",
    "    for i in range(len(response_mask)):\n",
    "        response_mask[i, input_size : response_end_idx[i]] = 1\n",
    "experience = [\n",
    "    {\n",
    "        \"input_sequence\": full_responses[\n",
    "            i, response_start_idx[i] : response_end_idx[i]\n",
    "        ],\n",
    "        \"log_probs\": log_probs[i, response_start_idx[i] : response_end_idx[i]],\n",
    "        \"response_mask\": response_mask[\n",
    "            i, response_start_idx[i] : response_end_idx[i]\n",
    "        ],\n",
    "        \"advantages\": advantages[i],\n",
    "    }\n",
    "    for i in range(advantages.shape[0])\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Step\n",
    "\n",
    "Run this block a couple of times and you should see the loss go down!\n",
    "If it doesn't, decrease the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "full_sequence = left_pad([b[\"input_sequence\"] for b in experience]).to(\n",
    "    accelerator.device\n",
    ")\n",
    "attention_mask = left_pad(\n",
    "    [torch.ones_like(b[\"input_sequence\"]) for b in experience], 0\n",
    ").to(accelerator.device)\n",
    "old_log_probs = left_pad([b[\"log_probs\"] for b in experience]).to(accelerator.device)\n",
    "response_mask = left_pad([b[\"response_mask\"] for b in experience]).to(accelerator.device)\n",
    "advantages = (\n",
    "    torch.cat([b[\"advantages\"] for b in experience], dim=0)\n",
    "    .unsqueeze(-1)\n",
    "    .to(accelerator.device)\n",
    ")\n",
    "\n",
    "log_probs = grpo_utils.calculate_logits(llm, full_sequence, attention_mask)\n",
    "\n",
    "loss = grpo_utils.calculate_grpo_loss(\n",
    "    log_probs=log_probs,\n",
    "    old_log_probs=old_log_probs,\n",
    "    response_mask=response_mask,\n",
    "    advantages=advantages,\n",
    ")\n",
    "print(loss)\n",
    "\n",
    "accelerator.backward(loss)\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def get_df(idx, clip_epsilon=1):\n",
    "    num_tokens = 25\n",
    "    importance_sampling_ratio = torch.exp(log_probs - old_log_probs)\n",
    "\n",
    "    unclipped_ratio = importance_sampling_ratio\n",
    "    clipped_ratio = torch.clamp(\n",
    "        importance_sampling_ratio, 1 - clip_epsilon, 1 + clip_epsilon\n",
    "    )\n",
    "\n",
    "    clipped_loss = clipped_ratio * advantages\n",
    "    unclipped_loss = unclipped_ratio * advantages\n",
    "\n",
    "    loss = -torch.min(unclipped_loss, clipped_loss)\n",
    "    tokens = [tokenizer.decode([token]) for token in full_sequence[idx, -num_tokens:]]\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"advantages\": advantages[idx].item(),\n",
    "            \"old_log_probs\": old_log_probs[idx, -num_tokens:].detach().cpu().float().numpy(),\n",
    "            \"log_probs\": log_probs[idx, -num_tokens:].detach().cpu().float().numpy(),\n",
    "            \"ratio\": importance_sampling_ratio[idx, -num_tokens:].detach().cpu().float().numpy(),\n",
    "            \"unclipped_ratio\": unclipped_ratio[idx, -num_tokens:].detach().cpu().float().numpy(),\n",
    "            f\"clipped_ratio (eps={clip_epsilon})\": clipped_ratio[idx, -num_tokens:].detach().cpu().float().numpy(),\n",
    "            \"unclipped_loss\": unclipped_loss[idx, -num_tokens:].detach().cpu().float().numpy(),\n",
    "            f\"clipped_loss (eps={clip_epsilon})\": clipped_loss[idx, -num_tokens:].detach().cpu().float().numpy(),\n",
    "            \"loss\": loss[idx, -num_tokens:].detach().cpu().float().numpy(),\n",
    "        }\n",
    "    )\n",
    "    df.index = tokens\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_df(0, clip_epsilon=0.2)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize log probs change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "def plot_log_probs(idx):\n",
    "    gap = 4\n",
    "    height = 1\n",
    "    num_tokens = 25\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    tokens = [tokenizer.decode([token]) for token in full_sequence[idx, -num_tokens:]]\n",
    "    plt.barh(\n",
    "        [gap*i for i in range(num_tokens)],\n",
    "        log_probs[idx, -num_tokens:].detach().cpu().float().numpy(),\n",
    "        label=\"log_probs\",\n",
    "        height=height,\n",
    "    )\n",
    "    plt.barh(\n",
    "        [gap*i+1 for i in range(num_tokens)],\n",
    "        old_log_probs[idx, -num_tokens:].detach().cpu().float().numpy(),\n",
    "        label=\"old_log_probs\",\n",
    "        height=height,\n",
    "    )\n",
    "    plt.xlabel(\"log_probs\")\n",
    "    plt.yticks(range(0, gap*num_tokens, gap), reversed(tokens))\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(f\"Advantage: {advantages[idx].item():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "idx = 0\n",
    "plot_log_probs(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(full_sequence[idx], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advantages[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
