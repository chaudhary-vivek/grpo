{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vivekchaudhary/anaconda3/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/vivekchaudhary/anaconda3/lib/python3.11/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <C1CC76AA-CD55-3E10-9064-29676E3E2535> /Users/vivekchaudhary/anaconda3/lib/python3.11/site-packages/torchvision/image.so\n",
      "  Expected in:     <B6BD92AE-4D03-3F92-9E03-2E2594A12866> /Users/vivekchaudhary/anaconda3/lib/python3.11/site-packages/torch/lib/libtorch_cpu.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from accelerate import Accelerator\n",
    "from utils import *\n",
    "import grpo_utils\n",
    "\n",
    "model_name = \"HuggingFaceTB/SmolLM-135M-Instruct\"\n",
    "batch_size = 2\n",
    "n_rollouts = 3\n",
    "buffer_size = 6\n",
    "max_new_tokens = 100\n",
    "\n",
    "\n",
    "# load essentials\n",
    "llm = load_model(model_name) # For full finetuning\n",
    "# llm = load_peft_model(model_name) # For only lora weights training\n",
    "tokenizer = load_tokenizer(model_name)\n",
    "dataloader = get_dataloader(\"syllogism\", tokenizer)\n",
    "optimizer = torch.optim.Adam(llm.parameters(), lr=1e-5)\n",
    "\n",
    "\n",
    "# Initialize accelerator\n",
    "accelerator = Accelerator()\n",
    "llm, tokenizer, dataloader, optimizer = accelerator.prepare(\n",
    "    llm, tokenizer, dataloader, optimizer\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['validator', 'inputs'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(dataloader))\n",
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch['validator']\n",
    "# batch['inputs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch[\"inputs\"][\"attention_mask\"]\n",
    "batch[\"inputs\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_end|><|im_end|><|im_start|>system\n",
      "\n",
      "A conversation between User and Assistant. The user asks a question, and the Assistant solves it.\n",
      "The assistant first thinks about the reasoning process in the mind and then provides the user\n",
      "with the answer. The reasoning process and answer are enclosed within <think> </think> and\n",
      "<answer> </answer> tags, respectively, i.e., <think> reasoning process here </think>\n",
      "<answer> answer here </answer>.\n",
      "\n",
      "Do not generate new code. Do not write python code.\n",
      "\n",
      "You may also be given examples by the user telling you the expected response format.\n",
      "Follow the format of the examples, but solve the specific problem asked by the user, not the examples.\n",
      "\n",
      "Very important - Remember again, your output format should be:\n",
      "<think> reasoning process here </think>\n",
      "<answer> answer here </answer>\n",
      "\n",
      "Your response will be scored by extracting the substring between the <answer>...</answer> tags.\n",
      "It is critical to follow the above format.\n",
      "feature_extraction_utilsling to follow the response format will result in a penalty.\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "Consider these statements:\n",
      "1. No students are humans\n",
      "2. All humans are chefs\n",
      "\n",
      "Does it logically follow that:\n",
      "Some chefs are humans?\n",
      "(Answer Yes or No)<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(batch[\"inputs\"][\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tokenizer.batch_decode(batch[\"inputs\"][\"input_ids\"])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Consider these statements:\\n1. All children are animals\\n2. Some animals are not doctors\\n\\nDoes it logically follow that:\\nSome children are not doctors?\\n(Answer Yes or No)', 'answer': 'Yes', 'metadata': {'source_dataset': 'syllogism', 'source_index': 1, 'premise1': 'All children are animals', 'premise2': 'Some animals are not doctors', 'conclusion': 'Some children are not doctors', 'is_valid': True, 'type': 'syllogism'}}\n"
     ]
    }
   ],
   "source": [
    "print(batch[\"validator\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "probability tensor contains either `inf`, `nan` or element < 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m input_size \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 8\u001b[0m     full_responses \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m      9\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m     10\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m     11\u001b[0m         max_new_tokens\u001b[38;5;241m=\u001b[39mmax_new_tokens, \u001b[38;5;66;03m# 100\u001b[39;00m\n\u001b[1;32m     12\u001b[0m         do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     13\u001b[0m         top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.95\u001b[39m,\n\u001b[1;32m     14\u001b[0m         num_return_sequences\u001b[38;5;241m=\u001b[39mn_rollouts,\n\u001b[1;32m     15\u001b[0m         temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     16\u001b[0m         eos_token_id\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39meos_token_id,\n\u001b[1;32m     17\u001b[0m     )\n\u001b[1;32m     19\u001b[0m     assistant_responses \u001b[38;5;241m=\u001b[39m full_responses[:, input_size:]\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# Calculate the logits for each selected tokens\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:2629\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2621\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2622\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2623\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2624\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2625\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2626\u001b[0m     )\n\u001b[1;32m   2628\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2629\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample(\n\u001b[1;32m   2630\u001b[0m         input_ids,\n\u001b[1;32m   2631\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[1;32m   2632\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[1;32m   2633\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[1;32m   2634\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[1;32m   2635\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[1;32m   2636\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2637\u001b[0m     )\n\u001b[1;32m   2639\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2640\u001b[0m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[1;32m   2641\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2642\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2643\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2644\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2645\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2646\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:3655\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3653\u001b[0m     probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(next_token_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   3654\u001b[0m     \u001b[38;5;66;03m# TODO (joao): this OP throws \"skipping cudagraphs due to ['incompatible ops']\", find solution\u001b[39;00m\n\u001b[0;32m-> 3655\u001b[0m     next_tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmultinomial(probs, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   3656\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3657\u001b[0m     next_tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(next_token_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: probability tensor contains either `inf`, `nan` or element < 0"
     ]
    }
   ],
   "source": [
    "batch = next(iter(dataloader))\n",
    "input_ids = batch[\"inputs\"][\"input_ids\"]\n",
    "attention_mask = batch[\"inputs\"][\"attention_mask\"]\n",
    "validator = batch[\"validator\"]\n",
    "input_size = input_ids.shape[1]\n",
    "\n",
    "with torch.no_grad():\n",
    "    full_responses = llm.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=max_new_tokens, # 100\n",
    "        do_sample=True,\n",
    "        top_p=0.95,\n",
    "        num_return_sequences=n_rollouts,\n",
    "        temperature=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    \n",
    "    assistant_responses = full_responses[:, input_size:]\n",
    "\n",
    "    # Calculate the logits for each selected tokens\n",
    "    log_probs = grpo_utils.calculate_logits(llm, full_responses, attention_mask)\n",
    "\n",
    "    # Convert tokens to string\n",
    "    decoded_responses = tokenizer.batch_decode(\n",
    "        assistant_responses, skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    # model_responses = [batch_size*n_rollouts, max_new_tokens]\n",
    "    rewards = grpo_utils.calculate_rewards(\n",
    "        decoded_responses, np.repeat(validator, n_rollouts)\n",
    "    )\n",
    "\n",
    "    # advantages = [batch_size, n_rollouts]\n",
    "    rewards = np.reshape(rewards, [batch_size, n_rollouts])\n",
    "    advantages = (rewards - np.mean(rewards, axis=1, keepdims=True)) / (\n",
    "        np.std(rewards, axis=1, keepdims=True) + 1e-8\n",
    "    )\n",
    "\n",
    "    advantages = advantages.reshape(-1, 1)\n",
    "    advantages = torch.tensor(advantages, dtype=torch.float32).to(llm.device)\n",
    "\n",
    "    padded_tokens = (full_responses != tokenizer.eos_token_id).int()\n",
    "    response_start_idx = padded_tokens.argmax(axis=-1)\n",
    "    response_end_idx = padded_tokens.shape[1] - torch.flip(\n",
    "        padded_tokens, dims=[1]\n",
    "    ).argmax(dim=1)\n",
    "\n",
    "    response_mask = torch.zeros_like(padded_tokens)\n",
    "    for i in range(len(response_mask)):\n",
    "        response_mask[i, input_size : response_end_idx[i]] = 1\n",
    "experience = [\n",
    "    {\n",
    "        \"input_sequence\": full_responses[\n",
    "            i, response_start_idx[i] : response_end_idx[i]\n",
    "        ],\n",
    "        \"log_probs\": log_probs[i, response_start_idx[i] : response_end_idx[i]],\n",
    "        \"response_mask\": response_mask[\n",
    "            i, response_start_idx[i] : response_end_idx[i]\n",
    "        ],\n",
    "        \"advantages\": advantages[i],\n",
    "    }\n",
    "    for i in range(advantages.shape[0])\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Step\n",
    "\n",
    "Run this block a couple of times and you should see the loss go down!\n",
    "If it doesn't, decrease the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "full_sequence = left_pad([b[\"input_sequence\"] for b in experience]).to(\n",
    "    accelerator.device\n",
    ")\n",
    "attention_mask = left_pad(\n",
    "    [torch.ones_like(b[\"input_sequence\"]) for b in experience], 0\n",
    ").to(accelerator.device)\n",
    "old_log_probs = left_pad([b[\"log_probs\"] for b in experience]).to(accelerator.device)\n",
    "response_mask = left_pad([b[\"response_mask\"] for b in experience]).to(accelerator.device)\n",
    "advantages = (\n",
    "    torch.cat([b[\"advantages\"] for b in experience], dim=0)\n",
    "    .unsqueeze(-1)\n",
    "    .to(accelerator.device)\n",
    ")\n",
    "\n",
    "log_probs = grpo_utils.calculate_logits(llm, full_sequence, attention_mask)\n",
    "\n",
    "loss = grpo_utils.calculate_grpo_loss(\n",
    "    log_probs=log_probs,\n",
    "    old_log_probs=old_log_probs,\n",
    "    response_mask=response_mask,\n",
    "    advantages=advantages,\n",
    ")\n",
    "print(loss)\n",
    "\n",
    "accelerator.backward(loss)\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def get_df(idx, clip_epsilon=1):\n",
    "    num_tokens = 25\n",
    "    importance_sampling_ratio = torch.exp(log_probs - old_log_probs)\n",
    "\n",
    "    unclipped_ratio = importance_sampling_ratio\n",
    "    clipped_ratio = torch.clamp(\n",
    "        importance_sampling_ratio, 1 - clip_epsilon, 1 + clip_epsilon\n",
    "    )\n",
    "\n",
    "    clipped_loss = clipped_ratio * advantages\n",
    "    unclipped_loss = unclipped_ratio * advantages\n",
    "\n",
    "    loss = -torch.min(unclipped_loss, clipped_loss)\n",
    "    tokens = [tokenizer.decode([token]) for token in full_sequence[idx, -num_tokens:]]\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"advantages\": advantages[idx].item(),\n",
    "            \"old_log_probs\": old_log_probs[idx, -num_tokens:].detach().cpu().float().numpy(),\n",
    "            \"log_probs\": log_probs[idx, -num_tokens:].detach().cpu().float().numpy(),\n",
    "            \"ratio\": importance_sampling_ratio[idx, -num_tokens:].detach().cpu().float().numpy(),\n",
    "            \"unclipped_ratio\": unclipped_ratio[idx, -num_tokens:].detach().cpu().float().numpy(),\n",
    "            f\"clipped_ratio (eps={clip_epsilon})\": clipped_ratio[idx, -num_tokens:].detach().cpu().float().numpy(),\n",
    "            \"unclipped_loss\": unclipped_loss[idx, -num_tokens:].detach().cpu().float().numpy(),\n",
    "            f\"clipped_loss (eps={clip_epsilon})\": clipped_loss[idx, -num_tokens:].detach().cpu().float().numpy(),\n",
    "            \"loss\": loss[idx, -num_tokens:].detach().cpu().float().numpy(),\n",
    "        }\n",
    "    )\n",
    "    df.index = tokens\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_df(0, clip_epsilon=0.2)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize log probs change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "def plot_log_probs(idx):\n",
    "    gap = 4\n",
    "    height = 1\n",
    "    num_tokens = 25\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    tokens = [tokenizer.decode([token]) for token in full_sequence[idx, -num_tokens:]]\n",
    "    plt.barh(\n",
    "        [gap*i for i in range(num_tokens)],\n",
    "        log_probs[idx, -num_tokens:].detach().cpu().float().numpy(),\n",
    "        label=\"log_probs\",\n",
    "        height=height,\n",
    "    )\n",
    "    plt.barh(\n",
    "        [gap*i+1 for i in range(num_tokens)],\n",
    "        old_log_probs[idx, -num_tokens:].detach().cpu().float().numpy(),\n",
    "        label=\"old_log_probs\",\n",
    "        height=height,\n",
    "    )\n",
    "    plt.xlabel(\"log_probs\")\n",
    "    plt.yticks(range(0, gap*num_tokens, gap), reversed(tokens))\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(f\"Advantage: {advantages[idx].item():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "idx = 0\n",
    "plot_log_probs(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(full_sequence[idx], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advantages[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
